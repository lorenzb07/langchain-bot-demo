from datetime import datetime

from langchain.agents import AgentExecutor, create_react_agent
from langchain_community.agent_toolkits.load_tools import load_tools
from langchain_core.prompts import PromptTemplate

from bot.models import ChatModelFactory
from bot.prompts import REACT_CHAT_PROMPT_TEMPLATE, REACT_PROMPT_TEMPLATE


def init_agent(
    provider: str,
    model_name: str,
    with_history: bool = False,
):
    """
    Initializes an agent with the specified model and optional history.

    Args:
        provider (str): The provider of the model to be used by
            the agent (e.g., "openai", "azure_openai", "groq").
        model_name (str): The name of the model to be used by the agent.
        with_history (bool): Whether to use a history-based prompt
            template. Defaults to False.

    Returns:
        AgentExecutor: An instance of AgentExecutor configured with
        the specified model, tools, and prompt.
    """
    if with_history:
        prompt = PromptTemplate.from_template(REACT_CHAT_PROMPT_TEMPLATE)
    else:
        prompt = PromptTemplate.from_template(REACT_PROMPT_TEMPLATE)

    tools = load_tools(["ddg-search", "read_file"])

    model = ChatModelFactory.get_llm(provider, model_name)

    agent = create_react_agent(model, tools, prompt)

    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=False,
        handle_parsing_errors=True,
    )


def run_agent(
    agent_executor: AgentExecutor,
    question: str,
):
    """
    Runs the agent with the specified model and question, streaming the output.

    Args:
        agent_executor (AgentExecutor): An instance of AgentExecutor configured
            with the model and tools to be used by the agent
        question (str): The input question to be processed by the agent.

    Returns:
        str: The output generated by the agent for the given question.
    """
    for chunk in agent_executor.stream(
        {
            "input": question,
            "today": datetime.now().strftime("%Y-%m-%d"),
        }
    ):
        print(chunk)
        print("----")
        if chunk.get("output"):
            return chunk["output"]

    return "No response"
